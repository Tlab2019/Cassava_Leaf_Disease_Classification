{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path as osp\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "import torch.utils.data as data\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import timm\n",
    "\n",
    "from bi_tempered_loss_pytorch import bi_tempered_logistic_loss\n",
    "from temperature_scaling import ModelWithTemperature\n",
    "\n",
    "# package_path = osp.join('..','input','image-fmix','FMix-master')\n",
    "# import sys; sys.path.append(package_path)\n",
    "# from fmix import sample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'rootpath': osp.join('..','input','cassava-leaf-disease-classification'),\n",
    "    'seed': 334,\n",
    "    'num_folds': 5,\n",
    "    \n",
    "    # augmentaion params\n",
    "    'aug': {\n",
    "        'ver': 'V3',\n",
    "        'size': 512,\n",
    "#         'size': 384,\n",
    "#         'size': 224,\n",
    "        'mean': (0.485, 0.456, 0.406),\n",
    "        'std': (0.229, 0.224, 0.225),\n",
    "#         'degrees': 45,\n",
    "        'degrees': 90,\n",
    "        'brightness': 0.3,\n",
    "        'contrast': 0.3,\n",
    "    },\n",
    "    \n",
    "    'do_mix': False,\n",
    "    'do_TemperatureScaling': False,\n",
    "    \n",
    "    # loss params\n",
    "    'loss': {\n",
    "#         'name': 'CrossEntropyLoss',\n",
    "        'name': 'BiTemperedLogisticLoss',\n",
    "        't1': 0.2,\n",
    "        't2': 1.2,\n",
    "#         't2': 4.0,\n",
    "#         'label_smoothing': 0.01,\n",
    "        'label_smoothing': 0.05,\n",
    "        'num_iters': 5,\n",
    "        'reduction': 'mean'\n",
    "    },\n",
    "\n",
    "#     'model_name': 'resnet50',\n",
    "#     'model_name': 'resnext50_32x4d',\n",
    "#     'model_name': 'seresnext50_32x4d',\n",
    "    'model_name': 'tf_efficientnet_b4_ns',\n",
    "    'batch_size': 8,\n",
    "    'test_batch_size': 1,\n",
    "    'num_epochs': 10,\n",
    "    'set_all': True,\n",
    "    \n",
    "    # optim params\n",
    "    'optim': {\n",
    "#         'name': 'SGD',\n",
    "        'name': 'Adam',\n",
    "        'lr': 1e-04,\n",
    "        'T_0': 10,   # CosineAnnealingWarmRestarts\n",
    "        'T_mult': 1,   # CosineAnnealingWarmRestarts\n",
    "        'min_lr': 1e-06,   # CosineAnnealingWarmRestarts\n",
    "        'last_epoch': -1,   # CosineAnnealingWarmRestarts\n",
    "        'weight_decay': 1e-06\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>..\\input\\cassava-leaf-disease-classification\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>..\\input\\cassava-leaf-disease-classification\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>..\\input\\cassava-leaf-disease-classification\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>..\\input\\cassava-leaf-disease-classification\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>..\\input\\cassava-leaf-disease-classification\\t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label                                         image_path\n",
       "0  1000015157.jpg      0  ..\\input\\cassava-leaf-disease-classification\\t...\n",
       "1  1000201771.jpg      3  ..\\input\\cassava-leaf-disease-classification\\t...\n",
       "2   100042118.jpg      1  ..\\input\\cassava-leaf-disease-classification\\t...\n",
       "3  1000723321.jpg      1  ..\\input\\cassava-leaf-disease-classification\\t...\n",
       "4  1000812911.jpg      3  ..\\input\\cassava-leaf-disease-classification\\t..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(osp.join(config['rootpath'], 'train.csv'))\n",
    "train['image_path'] = osp.join(config['rootpath'], 'train_images')\n",
    "train['image_path'] = train['image_path'].str.cat(train['image_id'], sep=osp.sep)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.read_csv(osp.join(config['rootpath'], 'sample_submission.csv'))\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform():\n",
    "    def __init__(self, resize, mean, std, degrees=45, brightness=0.3, contrast=0.3):\n",
    "        self.data_transform = {\n",
    "            'V1': {\n",
    "                'train': transforms.Compose([\n",
    "                    transforms.RandomResizedCrop(resize, scale=(0.5, 1.0)),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean, std)\n",
    "                ]),\n",
    "                'val': transforms.Compose([\n",
    "                    transforms.Resize(resize),\n",
    "                    transforms.CenterCrop(resize),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean, std)\n",
    "                ]),\n",
    "                'test': transforms.Compose([\n",
    "                    transforms.Resize(resize),\n",
    "                    transforms.CenterCrop(resize),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean, std)\n",
    "                ])                \n",
    "            },\n",
    "            'V2': {\n",
    "                'train': transforms.Compose([\n",
    "                    transforms.RandomResizedCrop(resize, scale=(0.5, 1.0)),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomVerticalFlip(),\n",
    "                    transforms.RandomRotation(degrees),\n",
    "                    transforms.ColorJitter(brightness, contrast),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean, std)\n",
    "                ]),\n",
    "                'val': transforms.Compose([\n",
    "                    transforms.Resize(resize),\n",
    "                    transforms.CenterCrop(resize),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean, std)\n",
    "                ]),\n",
    "                'test': transforms.Compose([\n",
    "                    transforms.Resize(resize),\n",
    "                    transforms.CenterCrop(resize),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean, std)\n",
    "                ])\n",
    "            },\n",
    "            'V3': {\n",
    "                'train': transforms.Compose([\n",
    "                    transforms.RandomResizedCrop(resize),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomVerticalFlip(),\n",
    "                    transforms.RandomRotation(degrees),\n",
    "                    transforms.ColorJitter(brightness, contrast),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean, std)\n",
    "                ]),\n",
    "                'val': transforms.Compose([\n",
    "                    transforms.Resize(resize),\n",
    "                    transforms.CenterCrop(resize),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean, std)\n",
    "                ]),\n",
    "                'test': transforms.Compose([\n",
    "                    transforms.Resize(resize),\n",
    "                    transforms.CenterCrop(resize),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean, std)\n",
    "                ])\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def __call__(self, img, ver='V1', phase='train'):\n",
    "        return self.data_transform[ver][phase](img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassavaDataset(data.Dataset):\n",
    "    def __init__(self, filepath2label, transform=None, ver='V1', phase='train', output_label=True):\n",
    "        self.file_list = list(filepath2label.keys())\n",
    "        self.transform = transform\n",
    "        self.filepath2label = filepath2label\n",
    "        self.ver = ver\n",
    "        self.phase = phase\n",
    "        self.output_label = output_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img, self.ver, self.phase)\n",
    "        if self.output_label:\n",
    "            label = self.filepath2label[img_path]\n",
    "            return img, label\n",
    "        else:\n",
    "            return img        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 動作確認\n",
    "\n",
    "# index = 0\n",
    "# print(train_dataset.__getitem__(index)[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DataLoader(dataset, batch_size, shuffle=True):\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 動作確認\n",
    "\n",
    "# batch_iterator = iter(dataloaders_dict['train'])\n",
    "# inputs, labels = next(batch_iterator)\n",
    "# print(inputs.size())\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNext50_32x4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNext(nn.Module):\n",
    "    def __init__(self, model_name='resnext50_32x4d', pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        n_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(n_features, 5, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet (b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEfficientNet(nn.Module):\n",
    "    def __init__(self, model_name='tf_efficientnet_b4_ns', pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, 5)\n",
    "        '''\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            #nn.Linear(n_features, hidden_size,bias=True), nn.ELU(),\n",
    "            nn.Linear(n_features, n_class, bias=True)\n",
    "        )\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(model_name, use_pretrained=True):\n",
    "    if model_name == 'vgg19':\n",
    "        net = models.vgg19(pretrained=use_pretrained)\n",
    "        net.classifier[6] = nn.Linear(in_features=4096, out_features=5, bias=True)\n",
    "        update_param_names = ['classifier.6.weight', 'classifier.6.bias']\n",
    "    elif model_name == 'resnet50':\n",
    "        net = models.resnet50(pretrained=use_pretrained)\n",
    "        net.fc = nn.Linear(in_features=2048, out_features=5, bias=True)\n",
    "        update_param_names = ['fc.weight', 'fc.bias']\n",
    "    elif model_name == 'resnext50_32x4d':\n",
    "        net = CustomResNext(model_name=model_name, pretrained=use_pretrained)\n",
    "        update_param_names = ['model.fc.weight', 'model.fc.bias']\n",
    "    elif model_name == 'seresnext50_32x4d':\n",
    "        net = CustomResNext(model_name=model_name, pretrained=use_pretrained)\n",
    "        update_param_names = ['model.fc.weight', 'model.fc.bias']\n",
    "    elif model_name == 'tf_efficientnet_b4_ns':\n",
    "        net = CustomEfficientNet(model_name=model_name, pretrained=use_pretrained)\n",
    "        update_param_names = ['model.classifier.weight', 'model.classifier.bias']\n",
    "    \n",
    "    return net, update_param_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiTemperedLogisticLoss(nn.Module):\n",
    "    def __init__(self, t1=0.8, t2=1.4, label_smoothing=0.0, num_iters=5, reduction='mean'):\n",
    "        super(BiTemperedLogisticLoss, self).__init__()\n",
    "        self.t1 = t1\n",
    "        self.t2 = t2\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.num_iters = num_iters\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        loss = bi_tempered_logistic_loss(outputs, labels, t1=self.t1, t2=self.t2, label_smoothing=self.label_smoothing, num_iters=self.num_iters, reduction=self.reduction)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_criterion(loss, params):\n",
    "    if loss == 'CrossEntropyLoss':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif loss == 'BiTemperedLogisticLoss':\n",
    "        criterion = BiTemperedLogisticLoss(params['t1'], params['t2'], params['label_smoothing'], params['num_iters'], params['reduction'])\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_params(net, update_param_names, set_all=False):\n",
    "    # add parameters to learn by fine-tuning to params_to_update\n",
    "    params_to_update = []\n",
    "\n",
    "    for name, param in net.named_parameters():\n",
    "        if set_all:\n",
    "            param.requires_grad = True\n",
    "            params_to_update.append(param)\n",
    "        else:\n",
    "            if name in update_param_names:\n",
    "                param.requires_grad = True\n",
    "                params_to_update.append(param)\n",
    "                print(name)\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "#     print(\"-----------------\")\n",
    "#     print(params_to_update)\n",
    "    \n",
    "    return params_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(optimizer_name, params_to_update):\n",
    "    # set optimizer\n",
    "    if optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(params=params_to_update, lr=config['optim']['lr'], momentum=0.9)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(params=params_to_update, lr=config['optim']['lr'], weight_decay=config['optim']['weight_decay'])\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer):\n",
    "#     if CFG.scheduler=='ReduceLROnPlateau':\n",
    "#         scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "#     elif CFG.scheduler=='CosineAnnealingLR':\n",
    "#         scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "#     elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "#         scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=config['optim']['T_0'], T_mult=config['optim']['T_mult'], eta_min=config['optim']['min_lr'], last_epoch=config['optim']['last_epoch'])\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cutmix & Fmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix(data, target, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_target = target[indices]\n",
    "\n",
    "    lam = np.clip(np.random.beta(alpha, alpha),0.3,0.4)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    new_data = data.clone()\n",
    "    new_data[:, :, bby1:bby2, bbx1:bbx2] = data[indices, :, bby1:bby2, bbx1:bbx2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "    targets = (target, shuffled_target, lam)\n",
    "\n",
    "    return new_data, targets\n",
    "\n",
    "def fmix(data, targets, alpha, decay_power, shape, max_soft=0.0, reformulate=False):\n",
    "    lam, mask = sample_mask(alpha, decay_power, shape, max_soft, reformulate)\n",
    "    #mask =torch.tensor(mask, device=device).float()\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "    x1 = torch.from_numpy(mask).to(device)*data\n",
    "    x2 = torch.from_numpy(1-mask).to(device)*shuffled_data\n",
    "    targets=(targets, shuffled_targets, lam)\n",
    "    \n",
    "    return (x1+x2), targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs, sheduler, device):\n",
    "    # move network to device\n",
    "    net.to(device)\n",
    "    \n",
    "    # try speeding up\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_valid_preds = np.array([])\n",
    "    best_valid_labels = np.array([])\n",
    "    \n",
    "    # LOOP: epoch\n",
    "    epoch_result_dict = {'train': {'loss': np.array([]), 'acc': np.array([])}, 'val': {'loss': np.array([]), 'acc': np.array([])}}\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('--------------------')\n",
    "        \n",
    "        # LOOP: train & valid at each epoch\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()   # set train mode\n",
    "            else:\n",
    "                net.eval()   # set validation mode\n",
    "            \n",
    "            epoch_loss = 0.0   # sum of epoch loss\n",
    "            epoch_corrects = 0   # number of epoch correctness\n",
    "            \n",
    "            epoch_valid_preds = np.array([])\n",
    "            epoch_valid_labels = np.array([])\n",
    "            \n",
    "            # training at epoch = 0 is omitted to confirm the verification performance when unlearned.\n",
    "#             if (epoch == 0) and (phase == 'train'):\n",
    "#                 continue\n",
    "            \n",
    "            # LOOP: mini-batch\n",
    "            for inputs, labels in dataloaders_dict[phase]:\n",
    "                # move inputs, labels to device\n",
    "                inputs = inputs.to(device).float()\n",
    "                labels = labels.to(device).long()\n",
    "                \n",
    "                if phase == 'train' and config['do_mix']:\n",
    "                    mix_decision = np.random.rand()\n",
    "                    if mix_decision < 0.25:\n",
    "                        inputs, labels = cutmix(inputs, labels, 1.)\n",
    "                    elif mix_decision >=0.25 and mix_decision < 0.5:\n",
    "                        inputs, labels = fmix(inputs, labels, alpha=1., decay_power=5., shape=(config['aug']['size'], config['aug']['size']))\n",
    "                    \n",
    "                # initialize optimizer\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    with autocast():\n",
    "                        outputs = net(inputs.float())\n",
    "                        \n",
    "                        if phase == 'train':\n",
    "                            if config['do_mix'] and mix_decision < 0.50:\n",
    "                                loss = criterion(outputs, labels[0]) * labels[2] + criterion(outputs, labels[1]) * (1. - labels[2])\n",
    "                            else:\n",
    "                                loss = criterion(outputs, labels)\n",
    "                            \n",
    "#                         loss = criterion(outputs, labels)   # calc loss\n",
    "                        _, preds = torch.max(outputs, 1)   # get predicted label\n",
    "\n",
    "                        # when train, run back propagation\n",
    "                        if phase == 'train':\n",
    "                            scaler.scale(loss).backward()\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                            # loss.backward()\n",
    "                            # optimizer.step()\n",
    "\n",
    "                        # calc iteration result\n",
    "                        epoch_loss += loss.item() * inputs.size(0)\n",
    "                        if phase == 'train' and config['do_mix'] and mix_decision < 0.5:\n",
    "                            epoch_corrects += torch.sum(preds == labels[0].data).item()\n",
    "                        else:\n",
    "                            epoch_corrects += torch.sum(preds == labels.data).item()\n",
    "                            \n",
    "                        if phase == 'val':\n",
    "                            epoch_valid_preds = np.append(epoch_valid_preds, preds.cpu().numpy())\n",
    "                            epoch_valid_labels = np.append(epoch_valid_labels, labels.data.cpu().numpy())\n",
    "\n",
    "                    \n",
    "            # scheduler\n",
    "#             if isinstance(scheduler, ReduceLROnPlateau):\n",
    "#                 scheduler.step(avg_val_loss)\n",
    "#             elif isinstance(scheduler, CosineAnnealingLR):\n",
    "#                 scheduler.step()\n",
    "#             elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "#                 scheduler.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # print loss & accuracy in each epoch\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = float(epoch_corrects) / len(dataloaders_dict[phase].dataset)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            epoch_result_dict[phase]['loss'] = np.append(epoch_result_dict[phase]['loss'], epoch_loss)\n",
    "            epoch_result_dict[phase]['acc'] = np.append(epoch_result_dict[phase]['acc'], epoch_acc)\n",
    "\n",
    "            if phase == 'val':\n",
    "                # plot current epoch result\n",
    "                plt.plot(range(len(epoch_result_dict['train']['loss'])), epoch_result_dict['train']['loss'], label='train loss', linewidth=2, marker='D')\n",
    "                plt.plot(range(len(epoch_result_dict['val']['loss'])), epoch_result_dict['val']['loss'], label='valid loss', linewidth=2, marker='D')\n",
    "                plt.xlabel('epoch')\n",
    "                plt.ylabel('loss')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "                # plot train & val ACC (each epoch)\n",
    "                plt.plot(range(len(epoch_result_dict['train']['acc'])), epoch_result_dict['train']['acc'], label='train acc', linewidth=2, marker='D')\n",
    "                plt.plot(range(len(epoch_result_dict['val']['acc'])), epoch_result_dict['val']['acc'], label='valid acc', linewidth=2, marker='D')\n",
    "                plt.xlabel('epoch')\n",
    "                plt.ylabel('acc')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc >= best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(net.state_dict())\n",
    "                \n",
    "                best_valid_preds = copy.copy(epoch_valid_preds)\n",
    "                best_valid_labels = copy.copy(epoch_valid_labels)\n",
    "\n",
    "    net.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # save valid preds & labels\n",
    "    valid_result = pd.DataFrame({\"pred\": best_valid_preds, \"label\": best_valid_labels})\n",
    "    \n",
    "    return net, epoch_result_dict, valid_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test_data(net, dataloader, device):\n",
    "    all_preds = np.array([], dtype=int)\n",
    "    net.eval()\n",
    "    for inputs in tqdm(dataloader):\n",
    "        inputs = inputs.to(device).float()\n",
    "        outputs = net(inputs)\n",
    "        _, preds = torch.max(outputs, 1)   # get predicted label\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        all_preds = np.append(all_preds, preds)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    # set random seed\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list(rootpath, phase='train'):\n",
    "    target_path = osp.join(rootpath, phase+'_images', '*.jpg')\n",
    "\n",
    "    path_list = []\n",
    "    for path in glob.glob(target_path):\n",
    "        path_list.append(path)\n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filename_info(title, ext, model_name, fold, aug_ver, loss_name, num_epochs, batch_size, size, set_all, lr, do_mix):\n",
    "    if loss_name == 'CrossEntropyLoss':\n",
    "        filename = title+'_'+model_name+'_fold'+str(fold)+'_'+aug_ver+'_loss'+loss_name+'_epoch'+str(num_epochs)+'_batchsize'+str(batch_size)+'_imgsize'+str(size)+'_allparams'+str(set_all)+'_lr'+str(lr)+'_Mix'+str(do_mix)+ext\n",
    "    elif loss_name == 'BiTemperedLogisticLoss':\n",
    "        filename = title+'_'+model_name+'_fold'+str(fold)+'_'+aug_ver+'_loss'+loss_name+'_t1'+str(config['loss']['t1'])+'_t2'+str(config['loss']['t2'])+'_labelsmoothing'+str(config['loss']['label_smoothing'])+'_epoch'+str(num_epochs)+'_batchsize'+str(batch_size)+'_imgsize'+str(size)+'_allparams'+str(set_all)+'_lr'+str(lr)+'_Mix'+str(do_mix)+ext\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use device:  cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "--------------------\n",
      "[0 4 3 4 2 3 4 4]\n",
      "[4 3 3 3 3 1 3 3]\n",
      "[4 4 1 2 3 3 0 3]\n",
      "[0 3 3 3 4 3 2 3]\n",
      "[2 4 3 3 4 1 3 3]\n",
      "[0 3 3 3 1 4 3 3]\n",
      "[3 3 2 3 1 0 0 3]\n",
      "[3 3 1 4 2 1 2 3]\n",
      "[3 4 2 3 3 3 4 3]\n",
      "[4 4 4 3 3 0 4 3]\n",
      "[2 3 3 0 3 3 4 1]\n",
      "[3 2 0 3 1 0 3 3]\n",
      "[3 3 3 2 3 4 0 3]\n",
      "[2 1 3 3 3 4 3 3]\n",
      "[3 4 3 3 3 3 3 2]\n",
      "[3 0 3 3 4 3 3 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:08<?, ?it/s]\n",
      "0it [00:09, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 4 3 3 3 3 0]\n",
      "[2 3 4 1 2 3 1 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-e6ce81d78dde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# train & valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_result_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'num_epochs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-325c82a4bde9>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(net, dataloaders_dict, criterion, optimizer, num_epochs, sheduler, device)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;31m# LOOP: mini-batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                 \u001b[1;31m# move inputs, labels to device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\owner\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\owner\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\owner\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\owner\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-b915fbb61a01>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_label\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath2label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-86dd3306b335>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img, ver, phase)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'V1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_transform\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mver\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\owner\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\owner\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \"\"\"\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\owner\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;31m# put it from HWC to CHW format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    set_random_seed(config['seed'])\n",
    "    \n",
    "    # device setting\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"use device: \", device)\n",
    "\n",
    "    # CV loop\n",
    "    folds = StratifiedKFold(n_splits=config['num_folds'], shuffle=True, random_state=config['seed']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in tqdm(enumerate(folds)):\n",
    "        time_start = time.time()\n",
    "#         if fold > 0:\n",
    "#             break\n",
    "        \n",
    "        # Dataset\n",
    "        train_filepath2label = dict(zip(train.loc[train_idx, :].image_path, train.loc[train_idx, :].label))\n",
    "        val_filepath2label = dict(zip(train.loc[val_idx, :].image_path, train.loc[val_idx, :].label))\n",
    "\n",
    "        train_dataset = CassavaDataset(train_filepath2label, ImageTransform(config['aug']['size'], config['aug']['mean'], config['aug']['std'], config['aug']['degrees'], config['aug']['brightness'], config['aug']['brightness']), config['aug']['ver'], 'train', True)\n",
    "        val_dataset = CassavaDataset(val_filepath2label, ImageTransform(config['aug']['size'], config['aug']['mean'], config['aug']['std'], config['aug']['degrees'], config['aug']['brightness'], config['aug']['brightness']), config['aug']['ver'], 'val', True)\n",
    "        \n",
    "        # DataLoader\n",
    "        train_dataloader = get_DataLoader(train_dataset, config['batch_size'], True)\n",
    "        val_dataloader = get_DataLoader(val_dataset, config['batch_size'], True)\n",
    "        dataloaders_dict = {'train': train_dataloader, 'val': val_dataloader}\n",
    "        \n",
    "        # get NetWork model\n",
    "        use_pretrained = True\n",
    "        net, update_param_names = get_network(config['model_name'], use_pretrained)\n",
    "        \n",
    "        # set train mode\n",
    "        net.train()\n",
    "        \n",
    "        # GradScaler\n",
    "        scaler = GradScaler() \n",
    "\n",
    "        # criterion\n",
    "        criterion = get_criterion(config['loss']['name'], config['loss'])\n",
    "\n",
    "        # get optimizer\n",
    "        params_to_update = set_params(net, update_param_names, config['set_all'])\n",
    "        optimizer = get_optimizer(config['optim']['name'], params_to_update)\n",
    "        \n",
    "        # scheduler\n",
    "        scheduler = get_scheduler(optimizer)\n",
    "        \n",
    "        # train & valid\n",
    "        net, epoch_result_dict, valid_result = train_model(net, dataloaders_dict, criterion, optimizer, config['num_epochs'], scheduler, device)\n",
    "        \n",
    "        elapsed_time = time.time() - time_start\n",
    "        print(\"Fold{} Running time:{}\".format(fold, elapsed_time) + \"[sec]\")\n",
    "        \n",
    "        # plot train & val loss (each epoch)\n",
    "        figname = make_filename_info('loss', '.png', config['model_name'], fold, config['aug']['ver'], config['loss']['name'], config['num_epochs'], config['batch_size'], config['aug']['size'], config['set_all'], config['optim']['lr'], config['do_mix'])\n",
    "        plt.plot(range(len(epoch_result_dict['train']['loss'])), epoch_result_dict['train']['loss'], label='train loss', linewidth=2, marker='D')\n",
    "        plt.plot(range(len(epoch_result_dict['val']['loss'])), epoch_result_dict['val']['loss'], label='valid loss', linewidth=2, marker='D')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(osp.join('fig', figname))\n",
    "        plt.show()\n",
    "\n",
    "        # plot train & val ACC (each epoch)\n",
    "        figname = make_filename_info('acc', '.png', config['model_name'], fold, config['aug']['ver'], config['loss']['name'], config['num_epochs'], config['batch_size'], config['aug']['size'], config['set_all'], config['optim']['lr'], config['do_mix'])\n",
    "        plt.plot(range(len(epoch_result_dict['train']['acc'])), epoch_result_dict['train']['acc'], label='train acc', linewidth=2, marker='D')\n",
    "        plt.plot(range(len(epoch_result_dict['val']['acc'])), epoch_result_dict['val']['acc'], label='valid acc', linewidth=2, marker='D')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('acc')\n",
    "        plt.legend()\n",
    "        plt.savefig(osp.join('fig', figname))\n",
    "        plt.show()\n",
    "\n",
    "        # save model\n",
    "        filename = make_filename_info('model', '.pth', config['model_name'], fold, config['aug']['ver'], config['loss']['name'], config['num_epochs'], config['batch_size'], config['aug']['size'], config['set_all'], config['optim']['lr'], config['do_mix'])\n",
    "        save_path = osp.join('trained_model', filename)\n",
    "        torch.save(net.state_dict(), save_path)\n",
    "        \n",
    "        # TemperatureScaling\n",
    "        if config['do_TemperatureScaling']:\n",
    "            net = ModelWithTemperature(net)\n",
    "            net.set_temperature(dataloaders_dict['val'])\n",
    "\n",
    "            # save model\n",
    "            filename = 'TemperatureScaling_' + filename\n",
    "            save_path = osp.join('trained_model', filename)\n",
    "            torch.save(scaled_model.model.state_dict(), save_path)\n",
    "            \n",
    "        # save valid preds\n",
    "        filename = make_filename_info('validpreds', '.csv', config['model_name'], fold, config['aug']['ver'], config['loss']['name'], config['num_epochs'], config['batch_size'], config['aug']['size'], config['set_all'], config['optim']['lr'], config['do_mix'])\n",
    "        save_path = osp.join('valid_preds', filename)\n",
    "        valid_result.to_csv(save_path, index=False)\n",
    "\n",
    "#     # prediction for test dataset\n",
    "#     test_datalist = make_datapath_list(config['rootpath'], 'test')\n",
    "#     test_filepath2label = dict(zip(test_datalist, [0] * len(test_datalist)))\n",
    "#     test_dataset = CassavaDataset(test_filepath2label, ImageTransform(config['aug']['size'], config['aug']['mean'], config['aug']['std'], config['aug']['degrees'], config['aug']['brightness'], config['aug']['brightness']), config['aug']['ver'], 'test', False)\n",
    "#     test_dataloader = get_DataLoader(test_dataset, config['batch_size'], False)\n",
    "    \n",
    "#     test_preds = inference_test_data(net, test_dataloader, device)\n",
    "    \n",
    "#     # make submission.csv\n",
    "#     submission = pd.DataFrame({'image_id': [osp.basename(k) for k in test_datalist], 'label': test_preds})\n",
    "#     submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
